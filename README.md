# NN_experiments
Могут ли нейросети хорошо обучаться на неправильно размеченных данных?
Может ли test accuracy быть больше train accuraty (overfit наоборот)?

Я провёл эксперимент:
1) Натренировал небольшую CNN на MNIST, причем 80% примеров были размечены случайным образом (из 9 других возможных цифр, т.е. правильный лэйбл не мог случайно выпасть).
2) И получил следующие результаты:
Training Accuracy: 18.66%
Test Accuracy: 93.50%

Это говорит о том, что нейросети могут улавливать истинные закономерности, знания даже из некачественных данных.
Я предполагаю, что это может обобщаться и на большие языковые модели, а именно, что LLMs способны превзойти уровень понимания и знания мира человека (получить более хорошую world model), несмотря на то, что они были натренированны исключительно на тексте, написанном людьми.

Результаты можно воспроизвести, random seed зафиксирован.
